{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "#### [CSCI-UA.0473 Spring 2020] Introduction to Machine Learning\n",
    "\n",
    "#### May 06 2020\n",
    "Prepared by Sean Welleck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.signal\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: find a **policy** $\\pi_{\\theta}(a|s)$ that maximizes the **expected total reward** of its **trajectories**.\n",
    "\n",
    "\\begin{align*}\n",
    "s\\in \\mathcal{S} & \\quad\\text{'state' in 'state space'}\\\\\n",
    "a\\in \\mathcal{A} & \\quad\\text{'action' in 'action space'}\\\\\n",
    "r\\in \\mathbb{R}  & \\quad\\text{'reward'}\\\\\n",
    "\\tau=(s_0, a_0, r_0, s_1, a_1, r_1,\\ldots, s_T) & \\quad\\text{trajectory (or 'episode')}\\\\\n",
    "R(\\tau) =\\sum_{t=0}^{T} r_t         & \\quad\\text{'total reward' (or 'return')}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\color{blue}{p(\\tau)} &= \\underbrace{p(s_0)}_{\\text{initial state distribution}}\\prod_{t=0}^{T-1} \\underbrace{\\pi_{\\theta}(a_t|s_t)}_{\\text{policy}}\\underbrace{p(s_{t+1}|a_t, s_t)}_{\\text{transition distribution}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\displaystyle J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau\\sim \\color{blue}{p(\\tau)}}\\left[R(\\tau)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "#### Policy Gradient\n",
    "\n",
    "Sample $K$ trajectories, then form the gradient estimator\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_{\\theta} &\\approx \\frac{1}{K}\\sum_{i=1}^{K} \\sum_{t=1}^{|\\tau|} \\underbrace{\\left(R_t - b(s_t^{(i)})\\right)}_{\\text{\"advantage\" A(s,a)}}\\nabla_{\\theta}\\log \\pi_{\\theta}(a^{(i)}_t|s_t^{(i)})\n",
    "\\end{align}\n",
    "\n",
    "$R_t$ can be the full return $R(\\tau)$ or $\\sum_{t'=t}^{|\\tau|}r_t$ (we will see specifics below)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup: One-step Toy Example\n",
    "\n",
    "**State**: $\\emptyset$ (single state)\n",
    "\n",
    "**Action**: $\\{1,2,\\ldots,A\\}$\n",
    "\n",
    "**Reward**: \n",
    "\\begin{align}\n",
    "r(s,a) &= \\begin{cases}\n",
    "    1.0 & a=1\\\\\n",
    "    0.9 & a\\in \\{2,\\ldots,\\frac{A}{2}\\}\\\\\n",
    "    0.0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "The policy must learn to choose $a = 1$ to maximize the reward ($1.0$).\n",
    "\n",
    "We will use **policy gradient**, and see how the **baseline** and **number of samples** affect the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward (unknown to the policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(object):\n",
    "    def __init__(self, num_actions):\n",
    "        rs = torch.zeros(num_actions)\n",
    "        rs[0] = 1.0\n",
    "        rs[1:int(num_actions//2)] = 0.90\n",
    "        self.rs = rs\n",
    "\n",
    "    def __call__(self, actions):\n",
    "        return self.rs[actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy\n",
    "\n",
    "The policy computes (the parameters of) a categorical distribution over actions, given a state.\n",
    "We parameterize this mapping with a neural network:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\pi_{\\theta}(a|s) &= \\text{softmax}(f_{\\theta}(s))\\\\\n",
    "    f_{\\theta}(s)&=W_2(\\text{tanh}(W_1s + b_1)) + b_2\n",
    "\\end{align*}\n",
    "\n",
    "*a neural network isn't needed for this simple problem, but we'll use a similar model below for a more complex problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_actions):\n",
    "        super(Policy, self).__init__()\n",
    "        self.w1 = nn.Linear(input_size, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.w1(state)\n",
    "        x = F.tanh(x)\n",
    "        log_ps = torch.log_softmax(self.w2(x), -1)\n",
    "        return log_ps\n",
    "    \n",
    "    def sample(self, log_ps):\n",
    "        actions = log_ps.exp().multinomial(1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Gradient\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_{\\theta} &\\approx \\frac{1}{K}\\sum_{i=1}^{K} \\underbrace{\\left(r(a^{(i)}) - b(s)\\right)}_{\\text{\"advantage\"} A(s,a)}\\nabla_{\\theta}\\log \\pi_{\\theta}(a^{(i)}|s)\n",
    "\\end{align}\n",
    "\n",
    "[NOTE:] To use the auto-differentiation library (PyTorch) in a standard way, we actually form a **loss function whose gradient is the policy gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(log_ps, actions, advantages):\n",
    "    policy_grad_loss = -torch.mean(advantages*log_ps.gather(1, actions))\n",
    "    return policy_grad_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "In this initial case, we will use **no baseline**, i.e.\n",
    "\n",
    "$$A(s, a) = r(a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 100\n",
    "num_samples = 1\n",
    "\n",
    "num_episodes = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(\n",
    "    input_size=1, \n",
    "    hidden_size=8, \n",
    "    num_actions=num_actions\n",
    ")\n",
    "reward_func = Reward(num_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "running_reward = 0\n",
    "for i in range(num_episodes):\n",
    "    state = torch.ones(num_samples, 1)\n",
    "    log_ps = policy(state)\n",
    "    actions = policy.sample(log_ps)\n",
    "    rewards = reward_func(actions)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pg_loss = loss(log_ps, actions, rewards)\n",
    "    pg_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for er in rewards:\n",
    "        running_reward = er.item() if running_reward is None else (\n",
    "            0.99 * running_reward + 0.01 * er.item())\n",
    "\n",
    "    if (i+1) % 1000 == 0 or i == 0:\n",
    "        print('Batch %d complete (episode %d), batch avg. reward: %.2f, running reward: %.3f' %\n",
    "              (i+1, (i+1)*num_samples, torch.mean(rewards).item(), running_reward))\n",
    "    \n",
    "        stats[('none', num_samples)].append(running_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running-average baseline\n",
    "\n",
    "Given a history of rewards from previous batches, $\\{r_1,\\ldots,r_N\\}$, let $b_{n} = 0.1r_{n-1} + 0.9 b_{n-1}$ (with $b_0=0,r_0=0$).\n",
    "\n",
    "$$A(s,a) = r(a) - b_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(\n",
    "    input_size=1, \n",
    "    hidden_size=8, \n",
    "    num_actions=num_actions\n",
    ")\n",
    "reward_func = Reward(num_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "running_reward = 0\n",
    "for i in range(num_episodes):\n",
    "    state = torch.ones(num_samples, 1)\n",
    "    log_ps = policy(state)\n",
    "    actions = policy.sample(log_ps)\n",
    "    rewards = reward_func(actions)\n",
    "    \n",
    "    # ------ baseline\n",
    "    advantages = torch.clamp(rewards - running_reward, min=0)\n",
    "    # ------\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pg_loss = loss(log_ps, actions, advantages)\n",
    "    pg_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for er in rewards:\n",
    "        running_reward = er.item() if running_reward is None else (\n",
    "            0.99 * running_reward + 0.01 * er.item())\n",
    "\n",
    "    if (i+1) % 1000 == 0 or i == 0:\n",
    "        print('Batch %d complete (episode %d), batch avg. reward: %.2f, running reward: %.3f' %\n",
    "              (i+1, (i+1)*num_samples, torch.mean(rewards).item(), running_reward))\n",
    "    \n",
    "        stats[('avg-baseline', num_samples)].append(running_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, figsize=(8, 5))\n",
    "\n",
    "axs.plot(stats[('none', num_samples)])\n",
    "axs.plot(stats[('avg-baseline', num_samples)])\n",
    "axs.hlines(0.90, -0.5, len(stats[('none', num_samples)])-1, alpha=0.8, linestyle=':')\n",
    "axs.hlines(1.0, -0.5, len(stats[('none', num_samples)])-1, alpha=0.8, linestyle=':')\n",
    "\n",
    "axs.set_xlim([-0.2, len(stats[('none', num_samples)])-1])\n",
    "axs.set_ylabel('reward')\n",
    "axs.set_xlabel('number of batches')\n",
    "axs.legend(['none', 'avg-baseline'], loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Atari Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "print(\"action space: %s\" % (env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(20, 10))\n",
    "\n",
    "state = env.reset()\n",
    "print(\"state size: %s\" % str(state.shape))\n",
    "\n",
    "axs[0].imshow(state)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('$s_0$', fontsize=20)\n",
    "\n",
    "for i in range(4):\n",
    "    action = 3 # up=2, down=3\n",
    "    state2, reward, done, info = env.step(action)  \n",
    "    \n",
    "    axs[i+1].imshow(state2)\n",
    "    axs[i+1].axis('off')\n",
    "    axs[i+1].set_title('$s_%d$' % (i+1), fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Episode with Random Policy\n",
    "\n",
    "In order to rollout a full trajectory to get a better sense of the problem, we'll use a simple random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    action = np.random.randint(6)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "rewards = []\n",
    "while not done:\n",
    "    action = random_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(rewards)\n",
    "print(\"episode length: %d\" % len(rewards))\n",
    "print(\"number of games: %d\" % (rewards != 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"episode rewards:\")\n",
    "i = 0\n",
    "for j, index in enumerate(rewards.nonzero()[0]):\n",
    "    print(\"\\tgame %d: %s\" % (j, ','.join(map(str, rewards[i:index+1].astype(int).tolist()))))\n",
    "    i = index+1\n",
    "    \n",
    "print(\"\\nreturn: %.1f\" % (rewards.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Recall from above that the policy gradient estimator weights the gradient of the log-probability by an 'advantage' term, which we can write as,\n",
    "\\begin{align}\n",
    "    \\nabla_{\\theta} \\approx \\frac{1}{K}\\sum_{k=1}^K\\sum_{t=1}^{|\\tau^{(k)}|}A(s_t^{(k)}, a_t^{(k)})\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t^{(k)}|s_t^{(k)})\n",
    "\\end{align}\n",
    "where $\\tau^{(k)} \\sim p(\\tau)$, and $\\tau^{(k)} = (s_0^{(k)}, a_0^{(k)}, r_0^{(k)},\\ldots,s_T^{(k)}).$\n",
    "\n",
    "#### Variance reduction - discounting\n",
    "\n",
    "Notice that the trajectories above are long. Consider the cumulative return of the first step (assuming the policy loses every game),\n",
    "\\begin{align}\n",
    "R(s_1,a_1)&=\\sum_{i=1}^{|\\tau|}r_t\\\\\n",
    "          &=-21.\n",
    "\\end{align}\n",
    "\n",
    "Intuitively, the first action has no effect on the reward $r_t$ for $t$ sufficiently far in the future, so we want to downweight rewards far in the future.\n",
    "In practice this is done through **discounting**,\n",
    "\\begin{align}\n",
    "    R(s_t,a_t) &= \\sum_{t'=t}^{|\\tau|} \\gamma^{t'-t}r_{t'}\n",
    "\\end{align}\n",
    "\n",
    "which effectively includes $\\frac{1}{1-\\gamma}$ timesteps in the sum.\n",
    "This introduces bias, but generally reduces variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _discount(rewards, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], rewards[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance reduction - learned baseline\n",
    "\n",
    "Next, we will introduce a **learned baseline**, which is known as a **state-value function** $V_{\\phi}(s_t)$, so that the advantage is,\n",
    "\\begin{align}\n",
    "    A(s_t,a_t) &= R(s_t,a_t) - V_{\\phi}(s_t).\n",
    "\\end{align}\n",
    "\n",
    "The state-value function $V_{\\phi}(s_t)$ will be trained to estimate the expected return at state $s_t$,\n",
    "\\begin{align}\n",
    "    \\displaystyle V_{\\phi}(s_t) &= \\mathop{\\mathbb{E}}_{a\\sim \\pi_{\\theta}}[R(s_t, a)]\n",
    "\\end{align}\n",
    "\n",
    "Intuitively, the advantage is thus positive when the observed return is 'higher than expected'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.w1(state)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses\n",
    "\n",
    "We train $V(s_t)$ with a regression loss ($L_2$ loss) using the observed returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vf_loss(values, returns):\n",
    "    loss = 0.5*torch.mean((values - returns)**2)\n",
    "    return loss\n",
    "\n",
    "def pg_loss(log_ps, actions, advantages, K):\n",
    "    loss = -torch.sum(log_ps.gather(1, actions) * advantages) / K\n",
    "    return loss\n",
    "\n",
    "def total_loss(log_ps, actions, values, returns, advantages, args):\n",
    "    actions = torch.tensor(actions, device=log_ps.device)\n",
    "    advantages = torch.tensor(advantages, device=log_ps.device)\n",
    "    returns = torch.tensor(returns, device=log_ps.device)\n",
    "    \n",
    "    entropy = -torch.sum(log_ps.exp()*log_ps, 1).mean()\n",
    "\n",
    "    loss = (\n",
    "        pg_loss(log_ps, actions, advantages, args.num_envs) +\n",
    "        args.vf_weight*vf_loss(values, returns) -\n",
    "        args.entropy_weight*entropy\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Network\n",
    "\n",
    "The policy now includes the value function; in practice, the value function's input is the hidden state computed by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_actions, gamma):\n",
    "        super(Policy, self).__init__()\n",
    "        self.w1 = nn.Linear(input_size, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, num_actions)\n",
    "        self.vf = ValueFunction(hidden_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.w1(state)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        values = self.vf(x)\n",
    "        log_ps = torch.log_softmax(self.w2(x), -1)\n",
    "        return log_ps, values\n",
    "\n",
    "    def act(self, log_ps):\n",
    "        actions = log_ps.argmax(dim=1)\n",
    "        return actions\n",
    "\n",
    "    def sample(self, log_ps):\n",
    "        actions = log_ps.exp().multinomial(1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode(policy, optimizer, envs, preprocessors, args):\n",
    "    \"\"\"Complete an episode's worth of training for each environment.\"\"\"\n",
    "    num_envs = len(envs)\n",
    "\n",
    "    # Buffers to hold trajectories, e.g. `env_xs[i]` will hold the observations for environment `i`.\n",
    "    env_xs, env_as = _2d_list(num_envs), _2d_list(num_envs)\n",
    "    env_rs, env_vs = _2d_list(num_envs), _2d_list(num_envs)\n",
    "    episode_rs = np.zeros(num_envs, dtype=np.float)\n",
    "\n",
    "    for p in preprocessors:\n",
    "        p.reset()\n",
    "    observations = [p.preprocess(e.reset()) for p, e in zip(preprocessors, envs)]\n",
    "\n",
    "    done = np.array([False for _ in range(num_envs)])\n",
    "    all_done = False\n",
    "    t = 1\n",
    "    while not all_done:\n",
    "        step_xs = np.vstack([o.ravel() for o in observations])\n",
    "\n",
    "        # Get actions and values for all environments in a single forward pass.\n",
    "        x = torch.tensor(step_xs, dtype=torch.float, device=args.device)\n",
    "        step_logps, step_vs = policy.forward(x)\n",
    "        step_as = policy.sample(step_logps)\n",
    "\n",
    "        # Step each environment whose episode has not completed.\n",
    "        for i, env in enumerate(envs):\n",
    "            if not done[i]:\n",
    "                obs, r, done[i], _ = env.step(step_as[i])\n",
    "\n",
    "                # Record the observation, action, value, and reward in the buffers.\n",
    "                env_xs[i].append(step_xs[i].ravel())\n",
    "                env_as[i].append(step_as[i])\n",
    "                env_vs[i].append(step_vs[i][0].item())\n",
    "                env_rs[i].append(r)\n",
    "                episode_rs[i] += r\n",
    "\n",
    "                # Add 0 as the state value when done.\n",
    "                if done[i]:\n",
    "                    env_vs[i].append(0.0)\n",
    "                else:\n",
    "                    observations[i] = preprocessors[i].preprocess(obs)\n",
    "\n",
    "        # Perform an update every `t_max` steps.\n",
    "        if t == args.t_max:\n",
    "            # If the episode has not finished, add current state's value. This will be used to\n",
    "            # 'bootstrap' the final return (see Algorithm S3 in A3C paper).\n",
    "            x = torch.tensor(np.vstack(observations), dtype=torch.float, device=args.device)\n",
    "            _, extra_vs = policy.forward(x)\n",
    "            for i in range(num_envs):\n",
    "                if not done[i]:\n",
    "                    env_vs[i].append(extra_vs[i][0].item())\n",
    "\n",
    "            # Perform update and clear buffers.\n",
    "            train_step(policy, optimizer, env_xs, env_as, env_rs, env_vs, args)\n",
    "            env_xs, env_as = _2d_list(num_envs), _2d_list(num_envs)\n",
    "            env_rs, env_vs = _2d_list(num_envs), _2d_list(num_envs)\n",
    "            t = 0\n",
    "\n",
    "        all_done = np.all(done)\n",
    "        t += 1\n",
    "\n",
    "    # Perform a final update when all episodes are finished.\n",
    "    if len(env_xs[0]) > 0:\n",
    "        train_step(policy, optimizer, env_xs, env_as, env_rs, env_vs, args)\n",
    "\n",
    "    return episode_rs\n",
    "\n",
    "def _2d_list(n):\n",
    "    return [[] for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(policy, optimizer, env_xs, env_as, env_rs, env_vs, args):\n",
    "    # Stack all the observations and actions.\n",
    "    xs = np.vstack(list(chain.from_iterable(env_xs)))\n",
    "    as_ = np.array(list(chain.from_iterable(env_as)))[:, np.newaxis]\n",
    "\n",
    "    # Compute discounted rewards and advantages.\n",
    "    drs, advs = [], []\n",
    "    gamma = args.gamma\n",
    "    for i in range(len(env_vs)):\n",
    "        # Compute discounted rewards with a 'bootstrapped' final value.\n",
    "        rs_bootstrap = [] if env_rs[i] == [] else env_rs[i] + [env_vs[i][-1]]\n",
    "        drs_ = _discount(rs_bootstrap, gamma)[:-1]\n",
    "        drs.extend(drs_)\n",
    "        advs.extend(drs_ - np.array(env_vs[i][:-1]))\n",
    "\n",
    "    drs = np.array(drs)[:, np.newaxis]\n",
    "    advs = np.array(advs)[:, np.newaxis]\n",
    "\n",
    "    # Forward and backward pass now that we have everything computed\n",
    "    optimizer.zero_grad()\n",
    "    xs = torch.tensor(xs, dtype=torch.float, device=args.device)\n",
    "    log_ps, vs = policy.forward(xs)\n",
    "    loss_ = total_loss(log_ps, as_, vs, drs, advs, args)\n",
    "    loss_.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), args.max_grad_norm)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari8080Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.prev = None\n",
    "        self.obs_size = 80*80\n",
    "\n",
    "    def reset(self):\n",
    "        self.prev = None\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        \"\"\" Preprocess a 210x160x3 uint8 frame into a 6400 (80x80) (1 x input_size) float vector.\"\"\"\n",
    "        # Crop, down-sample, erase background and set foreground to 1.\n",
    "        # Ref: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "        img = img[35:195]\n",
    "        img = img[::2, ::2, 0]\n",
    "        img[img == 144] = 0\n",
    "        img[img == 109] = 0\n",
    "        img[img != 0] = 1\n",
    "        curr = np.expand_dims(img.astype(np.float).ravel(), axis=0)\n",
    "        # Subtract the last preprocessed image.\n",
    "        diff = curr - self.prev if self.prev is not None else np.zeros((1, curr.shape[1]))\n",
    "        self.prev = curr\n",
    "        return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "We train the policy by repeatedly calling `train_episode`.\n",
    "A full implementation with command-line arguments is in `train.py`.\n",
    "\n",
    "It should take ~45 minutes on a macbook pro to solve the game (defined as a running reward of >20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': 0,\n",
    "    'num_envs': 16,\n",
    "    't_max': 50,\n",
    "    'env_type': 'PongDeterministic-v4',\n",
    "    'num_episodes': 1000000,\n",
    "    'print_every': 1,\n",
    "    'save_every': 5,\n",
    "    'learning_rate': 1e-3,\n",
    "    'gamma': 0.99,\n",
    "    'entropy_weight': 0.01,\n",
    "    'vf_weight': 2.0,\n",
    "    'hidden_size': 200,\n",
    "    'max_grad_norm': 1.0\n",
    "}\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\" dot.notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict(args)\n",
    "stats = defaultdict(list)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# Create and seed the environments\n",
    "envs = [gym.make(args.env_type) for _ in range(args.num_envs)]\n",
    "preprocessors = [Atari8080Preprocessor() for _ in range(args.num_envs)]\n",
    "\n",
    "input_size = preprocessors[0].obs_size\n",
    "num_actions = envs[0].action_space.n\n",
    "print(\"Input size %d\\nNum actions %d\" % (input_size, num_actions))\n",
    "\n",
    "for i, env in enumerate(envs):\n",
    "    env.seed(i+args.seed)\n",
    "\n",
    "policy = Policy(input_size, args.hidden_size, num_actions, args.gamma)\n",
    "policy.to(args.device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "# Train\n",
    "running_reward = None\n",
    "start = time.time()\n",
    "for i in range(args.num_episodes):\n",
    "    tic = time.time()\n",
    "    episode_rewards = train_episode(policy, optimizer, envs, preprocessors, args)\n",
    "\n",
    "    for er in episode_rewards:\n",
    "        running_reward = er if running_reward is None else (\n",
    "            0.99 * running_reward + 0.01 * er)\n",
    "\n",
    "    if i % args.print_every == 0:\n",
    "        print('Batch %d complete (%.2fs) (%.1fs elapsed) (episode %d), batch avg. reward: %.2f, running reward: %.3f' %\n",
    "              (i, time.time()-tic, time.time() - start, (i+1)*args.num_envs, np.mean(episode_rewards), running_reward))\n",
    "        stats['running_reward'].append(running_reward)\n",
    "        \n",
    "    if i % args.save_every == 0:\n",
    "        torch.save({\"state_dict\": policy.state_dict()}, \"model.pt\")\n",
    "        pickle.dump(stats, open(\"stats.pkl\", \"wb\"))\n",
    "        print(\"Model saved. (running reward %.2f)\" % running_reward)\n",
    "    \n",
    "    if running_reward >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "load_dir = './checkpoint'\n",
    "stats = pickle.load(open(os.path.join(load_dir, 'stats.pkl'), 'rb'))\n",
    "\n",
    "policy = Policy(input_size, args.hidden_size, num_actions, args.gamma)\n",
    "policy.load_state_dict(\n",
    "    torch.load(os.path.join(load_dir, 'model.pt'))['state_dict']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(args.env_type)\n",
    "pre = Atari8080Preprocessor()\n",
    "\n",
    "state = pre.preprocess(env.reset())\n",
    "done = False\n",
    "while not done:\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    log_ps, _ = policy(state)\n",
    "    action = policy.act(log_ps)[0].item()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    state = pre.preprocess(state)\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
